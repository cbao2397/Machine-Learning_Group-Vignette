---
title: "Group Vignette: Machine Learning"
author: "Lia Bao, Julianna Dick, Emiley Garcia-Zych, Cat Metcalf, Angelique Lindberg"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: cerulean
    toc: yes 
    toc_float: true
---
## Preliminaries 

> Install these packages in ***R:*** {caret}, {dplyr}, {ggplot2}, {rsample}

## Objectives

>The objective of this module is to demonstrate an applied example of machine learning via logistic regression. This module contains exercises to provide opportunities to apply machine learning.
>
> To complete this module, one is expected to have prior knowledge/familiarity with R syntax and logistic regression.

<center>
  <img src="https://github.com/cbao2397/Machine-Learning_Group-Vignette/blob/master/VirtualAssis.jpg?raw=true" alt="Siri, Alexa, Google." width=60%>
  <figcaption>Siri, Alexa, Google. (Image from https://www.gearpatrol.com/tech/a369922/how-does-a-virtual-assistant-work/)</figcaption>
</center>

## Introduction
> Do you use Siri, Alexa, Google, or Cortana? These are examples of artificial intelligence that rely on machine learning. Artificial intelligence refers to systems or machines that parallel human intelligence. Machine learning is a subdivision of artificial intelligence, and sometimes the terms are used interchangeably. It must be noted that, while all machine learning is artificial intelligence, not all artificial intelligence is machine learning. In addition to digital voice assistants, machine learning can be used for social network analytics, embedding interactive visual graphics, getting insight into the behavior of users, analyzing trends and patterns, and developing analytical solutions. Many of these applications are used by large companies such as Google, Facebook, Microsoft, and Amazon. 
>
> We see examples of machine learning in our everyday lives. On social media, recommendations of “people you may know,” targeted website ads for something you might’ve previously searched for or talked about, or when YouTube suggests another video saying “You May Also Like This” are all powered by machine learning. 

## What is Machine Learning?
> Machine learning is defined as developing systems that learn or improve performance based on the data they receive without being explicitly programmed. The term Machine Learning was first coined by Arthur Samuel in 1952 when he wrote the first IBM computer learning program for the game of checkers. As the computer played more checkers matches, it studied the moves and strategies and incorporated them into its program. Machine learning is classified into 3 categories: supervised learning, unsupervised learning, and reinforcement learning. In this module, we will focus on supervised and unsupervised learning.

### Supervised Learning
> As the name suggests, machine-supervised learning requires supervision in which we teach the machine using training data, labels, and target values (correct answers). The machine identifies variables and features within the training data and learns the algorithm that results in the target output. The machine models the relationships from the training data and uses these features to make predictions. One example of predictive supervised learning is using home attributes to predict real estate sales prices. Because we give the machine the training data and target outcomes, we are able to check the work of the machine’s predictions.

### Unsupervised Learning
> Unsupervised learning is often performed as part of exploratory data analysis. Unsupervised learning uses statistical tools to understand and describe the training data but target values are not provided. The machine must identify groups in the dataset. One example of unsupervised machine learning is identifying groups of online shoppers with similar browsing and purchasing histories. The machine groups these online shoppers and will show ads for products of their particular interest. In unsupervised machine learning, unlike in supervised learning, there is not one target outcome or answer because it depends on the data. As a result, one downside of unsupervised learning is that there’s no way to check your work because there is no true answer.

## Why Do Machine Learning?
> R has an expansive public domain of tools and packages to work with machine learning. Machine learning in R is useful for dealing with large amounts of data, it helps identify relationships in datasets to inform decision-making, and the machine can learn from itself to improve automatically. There are many different types of problems that machine learning can solve: regression, classification, association, clustering, anomaly detection, and recommendation. 

### General Process
> 1. Clean the data obtained from the training dataset  
> 2. Form a proper algorithm for building a prediction model  
> 3. Train the machine and model to understand the pattern of the dataset/project  
> 4. Predict your results more efficiently and with higher accuracy  

### What is Logistic Regression?
> The regression technique helps the machine predict continuous values, which is a type of supervised machine learning. Regression is a classic form of statistical modeling and one of the simplest tests for machine learning. Logistic regression is used to approximate the linear relationship between continuous variables and a set of continuous predictor variables. Using the slope and intercept of the training data’s regression, the machine can predict outcomes from given input or predictor variables. 

## Training the Model

>In order to produce a well-crafted, working model it is important to use the data wisely for the learning and validation procedures. This includes proper pre-processing for the feature and target variables while also minimizing data leakage. Finally, after you have completed this we can assess the model performance. 

>*Although we focus on supervised learning, these techniques can also be applied to the unsupervised learning later in the module.

>An example of how this model teaching works is shown below:

<center>
  <img src="https://github.com/cbao2397/Machine-Learning_Group-Vignette/blob/master/Model%20Diagram.jpg" alt="Model Diagram" width=60%>
  <figcaption>Modeling Diagram (Image from https://bradleyboehmke.github.io/HOML/index.html#software-information)</figcaption>
</center>

>This image shows the general predictive machine learning process where an optimal model is created by running the process repeatedly until it can be applied back to the testing data.

>We will be using the Heart Disease Dataset for teaching our model with supervised learning:

```{r}
library(curl)
f <- curl("https://raw.githubusercontent.com/cbao2397/DataStorage/main/moreprocessed.cleveland.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d) #loads in dataset
```


>There is some pre-processing we must conduct since there are missing values. The instructions for this are here, but we will also show them:

{pre-processing package load in}
{pre-processing code}

### Data Splitting

>One of the goals we have for machine learning is to find the algorithm f(X) that will accurately predict a future value or values (Yhat) based on a group of features (X). This algorithm fits past data but will also predict future outcomes using it’s learning from that previous data. This is called *generalizability*. Meaning how well we use our data to teach helps us understand how best the algorithm generalizes to unseen data. Think about our fight or flight response and how this became a learned adaptation. We first encountered something dangerous, and we either fought or fled. Let’s say someone encountered a bear in the woods for the first time, their body entered flight mode and they ran (Don’t actually do this if you encounter a bear!). They quickly discover running from the bear may lead to it chasing after them. Therefore, the next time they enter the woods, they may think about encountering a bear and they will likely not choose to flee, because that model did not work well for them, so they may alter the model to not flee but to make themselves look big and make really loud noises like dog barks, which successfully scares away the bear. The choice of fight or flight in this case is our *training set* and the final choice of model being to make ourselves big and bark like a dog is our *test set*. Basically:
**Training set**- used to develop groups of features that train the algorithms, tune hyperparameters, compare models, and other choices that lead to choosing a final model (Ex. the model that wants to be reproduced)
**Test set**- once the final model is chosen, this is the data used to estimate unbiased assessment of the model’s performance, which is also referred to as the *generalization error*. It is important this set does not get used while training your model since then it becomes part of the testing data and affects how well the model actually predicts data.

>Generally, the best split between the two is 60% training data and 40% testing data, because if you spend too much on training then you are not getting a valid assessment on the predictive performance (aka not generalizable). In contrast, too much spent on testing does not allow a good assessment of model parameters. Smaller training samples are often used for the sake of computation speed, but a general rule is p≥n (where p is the number of features and n is sample size). 

>Data can be split in two ways: Simple Random Sampling and Stratified Sampling

### Simple Random Sampling

>We have used random sampling in previous modules, so it follows a similar pattern here. Remember you can use set.seed() to make the sample reproducible. There are a few ways to do this, one in base R, one using the caret package that we use for the Heart Disease dataset, and one using the rsample package. It is largely dependent on what applies best to your chosen dataset.

>Let us take a 60-40 split through simple random sampling:

```{r}
library(caret)
library(rsample) #load in the necessary packages

# Using base R for random sampling
set.seed(123)  # for reproducibility of the random sample
index_1 <- sample(1:nrow(d), round(nrow(d) * 0.6)) #indicates we are taking from dataset d, by row and rounding and the 0.6 shows we are taking a 60-40 split
train_1 <- d[index_1, ] #creates our training set which we use for making the model
test_1  <- d[-index_1, ] #creates our test set which we will test our final model against

# Using caret package for random sampling
set.seed(123)  # for reproducibility
index_2 <- createDataPartition(d$age, p = 0.6, 
                               list = FALSE) #let us sample by age
train_2 <- d[index_2, ]
test_2  <- d[-index_2, ]

# Using rsample package for random sampling
set.seed(123)  # for reproducibility
split_1  <- initial_split(d, prop = 0.6)
train_3  <- training(split_1)
test_3   <- testing(split_1)
```


>This sampling size usually results in similar distributions of Y (Ex. Age in the Heart Disease dataset) between training and test sets.


### Stratified Sampling

>This type of sampling allows us to control our sample so we guarantee similar Y distributions. This is most commonly used with classification problems where a response variable is skewed (Ex. 90% of answers are “Yes” and 10% are “No”). This technique is also applied to regression problems of datasets with small sample sizes and where responses do not fall within a normal distribution. This implies this type of sampling will be best for our Heart Disease dataset, since it is based on classification, is multivariate, but likely does not follow a normal distribution.

>Since there is a continuous response variable, this sampling technique takes quantiles of the Y and randomly samples within these quantiles. This guarantees we are maintaining an equal representation across the data within our samples. 

{stratified sampling with rsample package?}
[]
#stratified sampling with the rsample package
set.seed(456)
split_strat  <- initial_split(d, prop = 0.7, 
                              strata = d$age) #splits the data in stratified units
train_strat  <- training(split_strat) #our training data
test_strat   <- testing(split_strat) #our testing data
] having trouble getting this to work!!!


>What do we think happens to our data if we did not take representative samples from each quantile?


### Creating models in R

>In order to fit our model, we need to identify the terms of the model. We can use symbolic representations of the terms, for example in previous modules we have seen y ~ x, meaning “y is a function of x”. We can also have separate arguments for predictors and outcomes, like previously seen with prop.test(x, n, p = NULL, alternative = c(“two.sided”, “less”, “greater”), conf.level = 0.95, correct = TRUE). Which technique you use will depend entirely on your dataset and usually a dataset within machine learning provides help documentation to point you to which technique to use and each technique has its limitations. For our dataset we will be using logistic regression to create a model. 
>Let us look into predicting fasting blood sugar based on age and then a second model based on sex. We will be using glm() like in previous models to evaluate this. Simply change the argument family="binomial" to tell the function to run a logistic regression (where it usually is family="gaussian"). This technique is evaluating predictions for a "Yes/No" value, expressed in the data set as a set of "0 1 0 0 1" values 

```{r}
library(ggplot2) #load in package to plot our predictions for our models
library(dplyr) #for transforming and interpreting our results later
model1 <- glm(fbs ~ age, family = "binomial", data = d)
model2 <- glm(fbs ~ sex, family = "binomial", data = d)
model3 <- glm(
  fbs ~ age + sex,
  family = "binomial", 
  data = d
  ) #uses multiple logistic regression to evaluate prediction by age AND sex
``` 
Issue with this: i wanted to do cholestrol instead of fbs (fasting blood sugar, but it needed a dataset with 0 1 0 1 1 (Ex) values for "Yes/No", so kind of weird?)
```{r}
exp(coef(model1))
exp(coef(model2))
exp(coef(model3)) 
```
*C: help with understanding interpretation???
As we can see the odds of fasting blood sugar being "Yes" multiplies for age by 1.039, where for sex in model 2 it multiplies by 1.35. Then for both age and sex in model 3, we see it multiplies by 1.04 and 1.47 for age and sex.

>Let's plot our models to see the predicted values for each feature representation to see the best fitting model:

```{r}
plot1 <- ggplot(data = model1, aes(x = age, y = fbs))
plot1 <- plot1 + geom_point()
plot1 <- plot1 + geom_smooth(method = "glm", formula = y ~ x)
plot1 #gives our plot, which will show the points of fbs Yes/No with the line representing the predicted values associated with age

plot2 <- ggplot(data = model2, aes(x = sex, y = fbs))
plot2 <- plot2 + geom_point()
plot2 <- plot2 + geom_smooth(method = "glm", formula = y ~ x)
plot2 #gives our plot, which will show the points of fbs Yes/No with the line representing the predicted values associated with sex

plot3 <- ggplot(data = model3, aes(x = age + sex, y = fbs))
plot3 <- plot3 + geom_point()
plot3 <- plot3 + geom_smooth(method = "glm", formula = y ~ x)
plot3 #gives our plot, which will show the points of fbs Yes/No with the line representing the predicted values associated with age and sex
```


### Resampling Methods

>We have split our dataset into training and testing sets, the test set once again not used in training our model and assessing its performance. In order to assess performance, the generalization, we need to apply resampling. This is called a validation approach, and it is similar to our data splitting section. We split the training set into to two new sets: a second training set and a validation set. We can then fit our model to segments of the training set and test its performance repeatedly.
The two common methods that apply this are k-fold cross validation and bootstrapping (which we have some experience doing previously). 

{resampling coding}


## Testing the Model

### Introduction
> Model testing is an important part in machine learning. It allows you to find problem or concern that may affect the prediction capability and accuracy. Knowing what and why the model is failing is always beneficial. Model testing and evaluation are very similar, while model testing usually perform certain tests aiming to test a particular issue in the model. 


### Testing Model
> 1. Calculate the model results to the data points in the testing data set. Use the test data as imput for the model to generate predictions. Perform this task using the highest performing model from the validation phase. You should have both the real values and the model's corresponding predictions for each input data instance in the data set. 
> 2. Compute statistical values comparing the model results to the test data. Check to ensure the model fits the test data set closely enough to be satisfactory. 